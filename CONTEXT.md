âœ… Project Name:

"AutoTagger" â€“ Serverless Metadata Tagging Pipeline on AWS
ğŸ” What Does This Project Accomplish?

This project creates a fully serverless, event-driven data tagging system on AWS.

When a file (e.g., a .json or .csv) is uploaded to an S3 bucket, a Python Lambda function is automatically triggered. It reads the file metadata (or content), adds custom tags, and stores results in a DynamoDB table or CloudWatch logs. The infrastructure is provisioned with Terraform, and tested with pytest using mocking libraries.

It simulates real-time automation and cloud-native resource processing, typical in enterprise-scale apps.
ğŸŒ Real-World Use Case Example

Imagine you're helping a government agency (since Publicis Sapient works with public sector clients) digitize thousands of scanned forms submitted by citizens. Your system:

    Automatically tags and classifies uploaded forms (e.g., application type, region, department).

    Validates each file format and logs issues.

    Writes this data into a searchable DynamoDB table.

    Is fully serverless, cost-efficient, and easy to scale.

ğŸ› ï¸ Project Roadmap
âœ… Phase 1: Set Up AWS & GitHub

Create a new GitHub repo: cloud-autotagger

Set up an AWS account (use Free Tier)

    Install & configure:

        AWS CLI

        Terraform

        Python environment (virtualenv)

        Git

ğŸ—ï¸ Phase 2: Infrastructure with Terraform

Create:

    main.tf, provider.tf, variables.tf, outputs.tf

Provision these AWS resources:

    âœ… S3 bucket (upload-autotagger-bucket)

    âœ… IAM role and policy for Lambda

    âœ… Lambda function to be triggered on S3 file uploads

    âœ… (Optional) DynamoDB table for structured output

ğŸ Phase 3: Write the Python Lambda Function

Create lambda_function.py:

Triggered by new .json or .csv uploads

Reads file metadata (size, format, timestamp)

(Optional) Parses content to extract key fields (e.g., "department": "housing")

    Logs output or writes it to DynamoDB

Use boto3 to interact with:

    S3 (read file)

    DynamoDB (optional write)

    CloudWatch (logging)

ğŸ§ª Phase 4: Add Unit Tests

Use pytest and moto (AWS mocking):

Test metadata extraction logic

Test behavior on missing fields or invalid files

    Mock S3/DynamoDB interactions

Directory structure:

cloud-autotagger/
â”œâ”€â”€ terraform/
â”œâ”€â”€ lambda/
â”‚   â”œâ”€â”€ lambda_function.py
â”‚   â”œâ”€â”€ requirements.txt
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_lambda.py

ğŸš€ Phase 5: CI/CD with GitHub Actions (Optional)

Create .github/workflows/deploy.yml to:

    Run pytest

    Zip & upload Lambda code

    Deploy Terraform automatically (or via manual trigger)

ğŸ“¦ Phase 6: Push to GitHub

Commands:

git init
git add .
git commit -m "Initial commit - AutoTagger project"
git branch -M main
git remote add origin https://github.com/your-username/cloud-autotagger.git
git push -u origin main

âœ… AWS Services Covered:

    S3 (event-driven triggers)

    Lambda (Python serverless function)

    IAM (permissions + roles)

    DynamoDB (optional database)

    CloudWatch (monitoring/logging)

    Terraform (infra as code)